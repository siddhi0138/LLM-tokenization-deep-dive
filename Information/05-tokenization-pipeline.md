# ğŸ”„ Tokenization Pipeline in Large Language Models

The **tokenization pipeline** defines the **exact sequence of transformations** that convert raw text into numerical representations usable by an LLM.

> **Key idea:**
> The model never sees text â€” it only sees numbers produced by this pipeline.

---

## 1ï¸âƒ£ High-Level Pipeline Overview

```
Text
  â†“
Normalization
  â†“
Pre-tokenization
  â†“
Subword Encoding
  â†“
Token IDs
  â†“
Embeddings
```

Each stage is **deterministic**, reproducible, and critical to model behavior.

---

## 2ï¸âƒ£ Stage 1: Text Normalization

### What Happens Here?

Text normalization ensures **consistent and clean input** before token splitting.

Common operations:

* Unicode normalization (NFKC / NFC)
* Standardizing whitespace
* Handling punctuation
* Case normalization (model-dependent)

### Example

```
Input:
â€œHello   WORLD!â€

After normalization:
"Hello WORLD!"
```

ğŸ“Œ **Why it matters**

* Prevents duplicate token forms
* Reduces vocabulary fragmentation
* Improves training efficiency

---

## 3ï¸âƒ£ Stage 2: Pre-tokenization

### Purpose

Pre-tokenization splits text into **coarse-grained units** before subword encoding.

Typical boundaries:

* Whitespace
* Punctuation
* Special characters

### Example

```
Input:
"Tokenization, at scale!"

Pre-tokenized:
["Tokenization", ",", "at", "scale", "!"]
```

ğŸ“Œ This stage **does not decide final tokens** â€” it only prepares candidates.

---

## 4ï¸âƒ£ Stage 3: Subword Encoding

### What Happens Here?

Pre-tokenized chunks are further broken down using:

* BPE
* WordPiece
* Unigram LM
* Byte-level encoding

The goal is to:

* Preserve frequent patterns
* Decompose rare words
* Minimize vocabulary size

### Example

```
"unhappiness"
â†’ ["un", "happi", "ness"]
```

ğŸ“Œ This is the **most important stage** of tokenization.

---

## 5ï¸âƒ£ Stage 4: Token to ID Mapping

Each final token is mapped to a **fixed integer ID** from the vocabulary.

### Example

```
Tokens:
["un", "happi", "ness"]

Token IDs:
[1532, 9841, 221]
```

ğŸ“Œ These IDs:

* Have no semantic meaning by themselves
* Are only indices into an embedding table

---

## 6ï¸âƒ£ Stage 5: Special Tokens Injection

Special tokens are added to structure the sequence.

Common additions:

* Beginning-of-sequence (`<BOS>`)
* End-of-sequence (`<EOS>`)
* Role tokens (`<SYSTEM>`, `<USER>`, `<ASSISTANT>`)

### Example

```
<BOS> un happi ness <EOS>
```

ğŸ“Œ These tokens strongly influence **model behavior**.

---

## 7ï¸âƒ£ Stage 6: Padding & Truncation

To support batching, sequences must have uniform length.

### Operations

* **Padding**: Add `<PAD>` tokens to shorter sequences
* **Truncation**: Cut sequences exceeding max length

### Example

```
Max length = 8

Input IDs:
[1, 1532, 9841, 221, 2]

Padded:
[1, 1532, 9841, 221, 2, 0, 0, 0]
```

---

## 8ï¸âƒ£ Stage 7: Embedding Lookup

Token IDs are converted into **dense vectors** using an embedding matrix.

### Example

```
Token ID â†’ â„^d
1532 â†’ [0.21, -0.14, ..., 0.37]
```

These embeddings:

* Capture token-level semantics
* Are trainable parameters
* Are combined with positional embeddings

ğŸ“Œ This is where **numbers become meaning**.

---

## 9ï¸âƒ£ Minimal Diagram: End-to-End Flow

![LLM Tokenization Pipeline](../images/LLM Tokenization Pipeline.png)

---


## ğŸ” Important Observations

* Tokenization is **deterministic**
* Same input â†’ same tokens (for a fixed tokenizer)
* Small text changes can produce very different token sequences
* Token count, not character count, determines cost and limits

---

## ğŸ§  Why the Pipeline Matters

This pipeline determines:

* How much context fits into the model
* How prompts are interpreted
* How efficiently the model runs
* How bias and multilingual behavior emerge

> **All intelligence in LLMs emerges downstream of this pipeline.**

---

## ğŸ”‘ Key Takeaways

* Tokenization is a multi-stage process, not a single step
* Subword encoding is the core stage
* Token IDs are meaningless without embeddings
* Small pipeline choices have large downstream effects

---

