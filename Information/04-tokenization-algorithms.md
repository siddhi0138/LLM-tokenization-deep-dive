# üß† Tokenization Algorithms in Large Language Models

Tokenization algorithms determine **how raw text is segmented into tokens** and how those tokens are selected for the model vocabulary.

Modern LLMs rely on **subword tokenization** to balance:

* Vocabulary size
* Sequence length
* Generalization
* Multilingual coverage

> **Key idea:**
> Tokenization algorithms compress language into statistically meaningful units that a model can efficiently learn.

---

## 1Ô∏è‚É£ Why Subword Tokenization?

Word-level tokenization leads to:

* Extremely large vocabularies
* Out-of-vocabulary (OOV) errors
* Poor handling of rare and compound words

Character-level tokenization leads to:

* Very long sequences
* Weak semantic signals
* Inefficient training

**Subword tokenization solves both problems** by decomposing rare words while keeping common patterns intact.

---

## 2Ô∏è‚É£ Overview of Tokenization Algorithms

| Algorithm                | Core Principle                | Used By    |
| ------------------------ | ----------------------------- | ---------- |
| Byte Pair Encoding (BPE) | Frequency-based merging       | GPT family |
| WordPiece                | Likelihood maximization       | BERT       |
| Unigram Language Model   | Probabilistic token selection | LLaMA, T5  |
| Byte-level Tokenization  | Tokenize raw bytes            | GPT-2+     |

---

## 3Ô∏è‚É£ Byte Pair Encoding (BPE)

### What is BPE?

**Byte Pair Encoding** is a **deterministic, frequency-based** tokenization algorithm.

It works by:

1. Starting with characters (or bytes)
2. Repeatedly merging the most frequent adjacent token pairs
3. Stopping when the target vocabulary size is reached

### Example

```
Corpus:
low lower lowest

After merges:
["low", "er", "est"]
```

### When to Use BPE

**Strengths**

* Simple and fast
* Efficient compression
* Strong performance for English and programming code

**Limitations**

* Frequency-based only (no probabilistic modeling)
* Less flexible for morphologically rich languages

üìå **Used in:** GPT-2, GPT-3, GPT-4 (byte-level BPE)

---

## 4Ô∏è‚É£ WordPiece

### What is WordPiece?

WordPiece is similar to BPE but differs in how merges are selected.

Instead of frequency, WordPiece:

* Chooses token splits that **maximize likelihood**
* Optimizes how well the tokens explain the training corpus

### Example

```
unbreakable
‚Üí un ##break ##able
```

The `##` prefix indicates continuation of a word.

### Strengths & Limitations

**Strengths**

* More linguistically consistent splits
* Better probabilistic modeling than BPE

**Limitations**

* More complex training
* Slightly slower than BPE

üìå **Used in:** BERT, DistilBERT, ALBERT

---

## 5Ô∏è‚É£ Unigram Language Model (SentencePiece)

### What is Unigram LM Tokenization?

Unigram tokenization takes a **top-down probabilistic approach**:

* Start with a large candidate vocabulary
* Assign probabilities to tokens
* Iteratively remove tokens that contribute least to likelihood

The final vocabulary maximizes:

```
P(text | tokens)
```

### Example

```
"unbelievable"

Possible segmentations:
["un","believable"]
["unbeliev","able"]
["un","bel","iev","able"]

‚Üí Highest-probability segmentation is selected
```

### Why It‚Äôs Important

**Strengths**

* Flexible token boundaries
* Excellent multilingual performance
* Robust to rare and unseen words

**Limitations**

* Slower training
* More complex inference

üìå **Used in:** LLaMA, T5, SentencePiece-based models

---

## 6Ô∏è‚É£ Byte-Level Tokenization

### What is Byte-Level Tokenization?

Instead of characters or words:

* Text is first converted into raw **UTF-8 bytes (0‚Äì255)**
* Tokenization is applied at the byte level

### Example

```
üòÄ ‚Üí [240, 159, 152, 128]
```

### Why Byte-Level Matters

**Strengths**

* No unknown tokens
* Handles emojis, symbols, code, and binary data
* Language-agnostic

**Limitations**

* Can increase token count for some text
* Less human-readable token splits

üìå **Used in:** GPT-2+, GPT-3, GPT-4

---

## 7Ô∏è‚É£ Minimal Diagram: Algorithm Flow (Where It Helps)

```
<img width="1024" height="1024" alt="Gemini_Generated_Image_dj0y5ddj0y5ddj0y" src="https://github.com/user-attachments/assets/86507fa3-7908-4cf7-888b-f6308cba607f" />

```

(Additional diagrams are unnecessary once the core concept is understood.)

---

## 8Ô∏è‚É£ Comparison Summary

| Feature              | BPE | WordPiece | Unigram LM | Byte-level |
| -------------------- | --- | --------- | ---------- | ---------- |
| Deterministic        | ‚úÖ   | ‚ùå         | ‚ùå          | ‚úÖ          |
| Probabilistic        | ‚ùå   | ‚úÖ         | ‚úÖ          | ‚ùå          |
| Handles OOV          | ‚ö†Ô∏è  | ‚ö†Ô∏è        | ‚ö†Ô∏è         | ‚úÖ          |
| Multilingual         | ‚ö†Ô∏è  | ‚ö†Ô∏è        | ‚úÖ          | ‚úÖ          |
| Code & Emoji Support | ‚úÖ   | ‚ö†Ô∏è        | ‚ö†Ô∏è         | ‚úÖ          |

---

## 9Ô∏è‚É£ Why This Choice Matters in LLMs

The tokenization algorithm directly influences:

* Token count per input
* Context window usage
* API cost
* Multilingual bias
* Model reasoning behavior

> **Tokenization is a core architectural decision, not a preprocessing step.**

---

## üîë Key Takeaways

* All modern LLMs use subword or byte-level tokenization
* BPE is fast and efficient
* WordPiece adds probabilistic rigor
* Unigram LM offers flexibility and multilingual strength
* Byte-level tokenization eliminates unknown tokens entirely

---
